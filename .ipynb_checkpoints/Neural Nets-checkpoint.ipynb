{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vanilla neural networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Motivation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's say you want to build a model that, given an image with a handwritten digit, you want it to **classify that digit**, *with a decent accuracy*. One of the most popular ways to do this is to use a **Vanilla neural network**. Let's dive in!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Structure of a neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Neural network](https://upload.wikimedia.org/wikipedia/commons/thumb/4/46/Colored_neural_network.svg/300px-Colored_neural_network.svg.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A neural network is a collection of *neurons* with *synapses* connecting them(think of it like a graph). The collection is organized in 3 big parts, each with their respective sizes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Layer type</th>\n",
       "      <th>Size(in, out)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Input</td>\n",
       "      <td>(InputSize, HiddenSize)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Hidden</td>\n",
       "      <td>(HiddenSize, OutputSize)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Output</td>\n",
       "      <td>(OutputSize, NumberOfClasses)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Layer type                  Size(in, out)\n",
       "0      Input        (InputSize, HiddenSize)\n",
       "1     Hidden       (HiddenSize, OutputSize)\n",
       "2     Output  (OutputSize, NumberOfClasses)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "data = [['Input', '(InputSize, HiddenSize)'], ['Hidden', '(HiddenSize, OutputSize)'], ['Output', '(OutputSize, NumberOfClasses)']]\n",
    "pd.DataFrame(data, columns=['Layer type', 'Size(in, out)'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Please note that there can be as many hidden layers as you want. In the example above I only used 1 layer. \"Deep\" learning implies multiple hidden layers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Moving forward, we will see how to *train* our neural network and how to get a prediction out of it. First, some notation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>Definition</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>X</td>\n",
       "      <td>Input vector</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>y</td>\n",
       "      <td>Output vector</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Name     Definition\n",
       "0    X   Input vector\n",
       "1    y  Output vector"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "data = [['X', 'Input vector'], ['y', 'Output vector']]\n",
    "pd.DataFrame(data, columns=['Name', 'Definition'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, our neural network takes as input a vector. How do we get this vector? We will use the MNIST dataset, which contains 70.000 28x28 images of handwritten digits."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The usual pipeline sounds like this:\n",
    "-  Grab the dataset\n",
    "-  Split it into training and testing batches\n",
    "-  Perform forward/backward propagation on the training set, while minimizing a loss function\n",
    "-  Test your accuracy on the testing batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Grabbing the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our implementation will be made using the PyTorch library - easy to use for begginers and also used by veterans. It already contains dataset loaders that will make our life easier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Processing...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "# A few imports\n",
    "from torchvision.datasets import MNIST\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "train_data = MNIST(root='./data', \n",
    "                   train=True, \n",
    "                   transform=torchvision.transforms.ToTensor(), \n",
    "                   download = True)\n",
    "\n",
    "test_data = MNIST(root='./data', \n",
    "                  train=False, \n",
    "                  transform=torchvision.transforms.ToTensor(), \n",
    "                  download=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data size: torch.Size([60000, 28, 28]), \n",
      "Labels: torch.Size([60000])\n",
      "########################################\n",
      "Test data size: torch.Size([10000, 28, 28]), \n",
      "Labels: torch.Size([10000, 28, 28])\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x288 with 10 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Take a look at the data\n",
    "print(f\"Train data size: {train_data.train_data.size()}, \\nLabels: {train_data.train_labels.size()}\")\n",
    "print('#' * 40)\n",
    "print(f\"Test data size: {test_data.test_data.size()}, \\nLabels: {test_data.test_data.size()}\")\n",
    "\n",
    "# Plot some examples from the training set\n",
    "w = 10\n",
    "h = 10\n",
    "fig = plt.figure(figsize=(8,4))\n",
    "columns = 5\n",
    "rows = 2\n",
    "for i in range(1, columns*rows + 1):\n",
    "    img = train_data.train_data[i]\n",
    "    fig.add_subplot(rows, columns, i)\n",
    "    plt.imshow(img, cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Network topology"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our input will be fed to the neural network by breaking the (28, 28) matrix that forms the image into a (28x28, 1) vector. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The neural network topology that we will use is the following:\n",
    "-  Input = 784 -> Hidden1 = 128\n",
    "-  Hidden1 -> ReLU\n",
    "-  Hidden1 = 128 -> Output = 10(number of classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training the network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training the neural networks takes place in two big parts:\n",
    "-  Forward propagation\n",
    "-  Backward propagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-  *1)* Initialize all the weights in the network with random numbers from a normal Gaussian distribution\n",
    "-  *2)* Give an input and propagate it all the way to the output\n",
    "-  *3)* Calculate the loss, with the loss function of your choice\n",
    "-  *4)* Calculate the gradient of the loss function with respect to each weight in the net\n",
    "-  *5)* Optimize the weights with a gradient descent step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, n_feature, n_hidden, n_output):\n",
    "        super(Net, self).__init__()\n",
    "        self.hidden = nn.Linear(n_feature, n_hidden)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.out = nn.Linear(n_hidden, n_output)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.hidden(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.out(x)\n",
    "        return x\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Net(28*28, 128, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Net(\n",
       "  (hidden): Linear(in_features=784, out_features=128, bias=True)\n",
       "  (relu): ReLU()\n",
       "  (out): Linear(in_features=128, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import SGD # The optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt_SGD = SGD(model.parameters(), lr = 0.001) #lr = learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss() # Loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(dataset=train_data, \n",
    "                                           batch_size=100,\n",
    "                                           shuffle = True)\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_data, \n",
    "                                           batch_size=100,\n",
    "                                           shuffle = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alex/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:27: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/20], Step [100/600], Loss: 2.2509\n",
      "Epoch [1/20], Step [200/600], Loss: 2.2324\n",
      "Epoch [1/20], Step [300/600], Loss: 2.1879\n",
      "Epoch [1/20], Step [400/600], Loss: 2.1935\n",
      "Epoch [1/20], Step [500/600], Loss: 2.1645\n",
      "Epoch [1/20], Step [600/600], Loss: 2.1477\n",
      "Epoch [2/20], Step [100/600], Loss: 2.1086\n",
      "Epoch [2/20], Step [200/600], Loss: 2.0719\n",
      "Epoch [2/20], Step [300/600], Loss: 2.0946\n",
      "Epoch [2/20], Step [400/600], Loss: 2.0519\n",
      "Epoch [2/20], Step [500/600], Loss: 2.0243\n",
      "Epoch [2/20], Step [600/600], Loss: 1.9864\n",
      "Epoch [3/20], Step [100/600], Loss: 1.9710\n",
      "Epoch [3/20], Step [200/600], Loss: 1.9343\n",
      "Epoch [3/20], Step [300/600], Loss: 1.9396\n",
      "Epoch [3/20], Step [400/600], Loss: 1.8458\n",
      "Epoch [3/20], Step [500/600], Loss: 1.8581\n",
      "Epoch [3/20], Step [600/600], Loss: 1.7885\n",
      "Epoch [4/20], Step [100/600], Loss: 1.7504\n",
      "Epoch [4/20], Step [200/600], Loss: 1.6931\n",
      "Epoch [4/20], Step [300/600], Loss: 1.7593\n",
      "Epoch [4/20], Step [400/600], Loss: 1.6767\n",
      "Epoch [4/20], Step [500/600], Loss: 1.6977\n",
      "Epoch [4/20], Step [600/600], Loss: 1.6041\n",
      "Epoch [5/20], Step [100/600], Loss: 1.6162\n",
      "Epoch [5/20], Step [200/600], Loss: 1.5540\n",
      "Epoch [5/20], Step [300/600], Loss: 1.5365\n",
      "Epoch [5/20], Step [400/600], Loss: 1.5571\n",
      "Epoch [5/20], Step [500/600], Loss: 1.5057\n",
      "Epoch [5/20], Step [600/600], Loss: 1.4693\n",
      "Epoch [6/20], Step [100/600], Loss: 1.3924\n",
      "Epoch [6/20], Step [200/600], Loss: 1.3930\n",
      "Epoch [6/20], Step [300/600], Loss: 1.3219\n",
      "Epoch [6/20], Step [400/600], Loss: 1.3517\n",
      "Epoch [6/20], Step [500/600], Loss: 1.2347\n",
      "Epoch [6/20], Step [600/600], Loss: 1.3074\n",
      "Epoch [7/20], Step [100/600], Loss: 1.1829\n",
      "Epoch [7/20], Step [200/600], Loss: 1.2794\n",
      "Epoch [7/20], Step [300/600], Loss: 1.1709\n",
      "Epoch [7/20], Step [400/600], Loss: 1.2210\n",
      "Epoch [7/20], Step [500/600], Loss: 1.1831\n",
      "Epoch [7/20], Step [600/600], Loss: 1.1079\n",
      "Epoch [8/20], Step [100/600], Loss: 1.1637\n",
      "Epoch [8/20], Step [200/600], Loss: 1.0038\n",
      "Epoch [8/20], Step [300/600], Loss: 1.0590\n",
      "Epoch [8/20], Step [400/600], Loss: 1.0312\n",
      "Epoch [8/20], Step [500/600], Loss: 1.0192\n",
      "Epoch [8/20], Step [600/600], Loss: 0.9932\n",
      "Epoch [9/20], Step [100/600], Loss: 1.1542\n",
      "Epoch [9/20], Step [200/600], Loss: 0.9648\n",
      "Epoch [9/20], Step [300/600], Loss: 1.0597\n",
      "Epoch [9/20], Step [400/600], Loss: 1.0355\n",
      "Epoch [9/20], Step [500/600], Loss: 0.9084\n",
      "Epoch [9/20], Step [600/600], Loss: 0.8196\n",
      "Epoch [10/20], Step [100/600], Loss: 0.8595\n",
      "Epoch [10/20], Step [200/600], Loss: 0.7675\n",
      "Epoch [10/20], Step [300/600], Loss: 0.8820\n",
      "Epoch [10/20], Step [400/600], Loss: 0.8837\n",
      "Epoch [10/20], Step [500/600], Loss: 0.8937\n",
      "Epoch [10/20], Step [600/600], Loss: 0.8063\n",
      "Epoch [11/20], Step [100/600], Loss: 0.8602\n",
      "Epoch [11/20], Step [200/600], Loss: 0.8533\n",
      "Epoch [11/20], Step [300/600], Loss: 0.8115\n",
      "Epoch [11/20], Step [400/600], Loss: 0.8263\n",
      "Epoch [11/20], Step [500/600], Loss: 0.7922\n",
      "Epoch [11/20], Step [600/600], Loss: 0.7051\n",
      "Epoch [12/20], Step [100/600], Loss: 0.7311\n",
      "Epoch [12/20], Step [200/600], Loss: 0.7667\n",
      "Epoch [12/20], Step [300/600], Loss: 0.6965\n",
      "Epoch [12/20], Step [400/600], Loss: 0.6803\n",
      "Epoch [12/20], Step [500/600], Loss: 0.8226\n",
      "Epoch [12/20], Step [600/600], Loss: 0.7329\n",
      "Epoch [13/20], Step [100/600], Loss: 0.7160\n",
      "Epoch [13/20], Step [200/600], Loss: 0.6980\n",
      "Epoch [13/20], Step [300/600], Loss: 0.7597\n",
      "Epoch [13/20], Step [400/600], Loss: 0.6373\n",
      "Epoch [13/20], Step [500/600], Loss: 0.6019\n",
      "Epoch [13/20], Step [600/600], Loss: 0.6136\n",
      "Epoch [14/20], Step [100/600], Loss: 0.5187\n",
      "Epoch [14/20], Step [200/600], Loss: 0.6666\n",
      "Epoch [14/20], Step [300/600], Loss: 0.6945\n",
      "Epoch [14/20], Step [400/600], Loss: 0.7147\n",
      "Epoch [14/20], Step [500/600], Loss: 0.6405\n",
      "Epoch [14/20], Step [600/600], Loss: 0.6578\n",
      "Epoch [15/20], Step [100/600], Loss: 0.5632\n",
      "Epoch [15/20], Step [200/600], Loss: 0.6034\n",
      "Epoch [15/20], Step [300/600], Loss: 0.6533\n",
      "Epoch [15/20], Step [400/600], Loss: 0.7562\n",
      "Epoch [15/20], Step [500/600], Loss: 0.7490\n",
      "Epoch [15/20], Step [600/600], Loss: 0.5827\n",
      "Epoch [16/20], Step [100/600], Loss: 0.6577\n",
      "Epoch [16/20], Step [200/600], Loss: 0.5359\n",
      "Epoch [16/20], Step [300/600], Loss: 0.6118\n",
      "Epoch [16/20], Step [400/600], Loss: 0.7295\n",
      "Epoch [16/20], Step [500/600], Loss: 0.6758\n",
      "Epoch [16/20], Step [600/600], Loss: 0.6461\n",
      "Epoch [17/20], Step [100/600], Loss: 0.6242\n",
      "Epoch [17/20], Step [200/600], Loss: 0.5669\n",
      "Epoch [17/20], Step [300/600], Loss: 0.7089\n",
      "Epoch [17/20], Step [400/600], Loss: 0.5543\n",
      "Epoch [17/20], Step [500/600], Loss: 0.4471\n",
      "Epoch [17/20], Step [600/600], Loss: 0.5171\n",
      "Epoch [18/20], Step [100/600], Loss: 0.6233\n",
      "Epoch [18/20], Step [200/600], Loss: 0.6451\n",
      "Epoch [18/20], Step [300/600], Loss: 0.6101\n",
      "Epoch [18/20], Step [400/600], Loss: 0.5240\n",
      "Epoch [18/20], Step [500/600], Loss: 0.4623\n",
      "Epoch [18/20], Step [600/600], Loss: 0.5529\n",
      "Epoch [19/20], Step [100/600], Loss: 0.5510\n",
      "Epoch [19/20], Step [200/600], Loss: 0.4772\n",
      "Epoch [19/20], Step [300/600], Loss: 0.5482\n",
      "Epoch [19/20], Step [400/600], Loss: 0.5063\n",
      "Epoch [19/20], Step [500/600], Loss: 0.4678\n",
      "Epoch [19/20], Step [600/600], Loss: 0.5245\n",
      "Epoch [20/20], Step [100/600], Loss: 0.4702\n",
      "Epoch [20/20], Step [200/600], Loss: 0.4614\n",
      "Epoch [20/20], Step [300/600], Loss: 0.4366\n",
      "Epoch [20/20], Step [400/600], Loss: 0.5965\n",
      "Epoch [20/20], Step [500/600], Loss: 0.4560\n",
      "Epoch [20/20], Step [600/600], Loss: 0.5836\n"
     ]
    }
   ],
   "source": [
    "from torch.autograd import Variable\n",
    "\n",
    "num_epochs = 20\n",
    "batch_size = 100\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        # Convert tensors to variables\n",
    "        images = Variable(images.view(-1, 28*28))\n",
    "        labels = Variable(labels)\n",
    "\n",
    "        opt_SGD.zero_grad() # Zero out all gradients\n",
    "\n",
    "        # Calculate the output\n",
    "        outputs = model(images)\n",
    "\n",
    "        # Calculate the loss function based on the output and the truth\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "\n",
    "        # Update the weights\n",
    "        opt_SGD.step()\n",
    "\n",
    "        if (i + 1) % 100 == 0:\n",
    "            print('Epoch [%d/%d], Step [%d/%d], Loss: %.4f'\n",
    "                     %(epoch+1, num_epochs, i+1, len(train_data)//batch_size, loss.data[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 87 %\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "for images, labels in test_loader:\n",
    "    images = Variable(images.view(-1, 28*28))\n",
    "    outputs = model(images)\n",
    "    _, predicted = torch.max(outputs.data, 1)  # Choose the best class from the output: The class with the best score\n",
    "    total += labels.size(0)                    # Increment the total count\n",
    "    correct += (predicted == labels).sum()     # Increment the correct count\n",
    "    \n",
    "print('Accuracy: %d %%' % (100 * correct / total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To save the trained model:\n",
    "torch.save(model.state_dict(), 'model.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problems with vanilla DNNs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The problem with vanilla DNNs is that they don't scale well with the size of the picture. In order to take as input a 28x28 picture, we had to keep in memory and train a bunch of weights, even for a very basic topology! Now imagine going up in size with a 1920x1080 picture or something along these lines. From 28x28 = 784 we go to 1920x1080 = 2073600. It's simply not doable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolutional neural networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3D volumes of neurons. Convolutional Neural Networks take advantage of the fact that the input consists of images and they constrain the architecture in a more sensible way. In particular, unlike a regular Neural Network, the layers of a ConvNet have neurons arranged in 3 dimensions: width, height, depth. (Note that the word depth here refers to the third dimension of an activation volume, not to the depth of a full Neural Network, which can refer to the total number of layers in a network.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![CNN](http://cs231n.github.io/assets/nn1/neural_net2.jpeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![CNN2](http://cs231n.github.io/assets/cnn/cnn.jpeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A simple ConvNet is a sequence of layers, and every layer of a ConvNet transforms one volume of activations to another through a differentiable function. We use three main types of layers to build ConvNet architectures: Convolutional Layer, Pooling Layer, and Fully-Connected Layer (exactly as seen in regular Neural Networks). We will stack these layers to form a full ConvNet architecture."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is an example on what the CNNs learn:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![CNNLayers](http://cs231n.github.io/assets/cnn/weights.jpeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![ASD](http://cs231n.github.io/assets/cnn/convnet.jpeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'trainloader' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-fa9ed39cf7c8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     49\u001b[0m                     100. * batch_index / len(trainloader), loss.data[0]))\n\u001b[1;32m     50\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m# loop over the dataset multiple times\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m     \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-4-fa9ed39cf7c8>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(epoch)\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mbatch_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m         \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'trainloader' is not defined"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "from torchvision import datasets, transforms\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 10, kernel_size=5)\n",
    "        self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\n",
    "        self.conv2_drop = nn.Dropout2d()\n",
    "        self.fc1 = nn.Linear(320, 120)\n",
    "        self.fc1_drop = nn.Dropout()\n",
    "        self.fc2 = nn.Linear(120, 10)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(F.max_pool2d(self.conv1(x), 2))\n",
    "        x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2))\n",
    "        x = x.view(-1, self.num_flat_features(x))\n",
    "        x = F.relu(self.fc1_drop(self.fc1(x)))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        return F.log_softmax(x)\n",
    "    \n",
    "    def num_flat_features(self, x):\n",
    "        size = x.size()[1:] # all dimensions except the batch dimension\n",
    "        num_features = 1\n",
    "        for s in size:\n",
    "            num_features *= s\n",
    "        return num_features\n",
    "\n",
    "net = Net()\n",
    "optimizer = optim.Adam(net.parameters())\n",
    "def train(epoch):\n",
    "    net.train()\n",
    "    for batch_index, (inputs, labels) in enumerate(trainloader):\n",
    "        inputs, labels = Variable(inputs), Variable(torch.squeeze(labels))\n",
    "        optimizer.zero_grad()\n",
    "        outputs = net(inputs)\n",
    "        loss = F.nll_loss(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_index % 400 == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                    epoch, batch_index * len(inputs), len(trainloader.dataset),\n",
    "                    100. * batch_index / len(trainloader), loss.data[0]))\n",
    "for epoch in range(4): # loop over the dataset multiple times\n",
    "    train(epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
